{"paragraphs":[{"text":"import java.io.File\r\nimport scala.io.Source\r\n\r\nimport org.apache.log4j.Logger\r\nimport org.apache.log4j.Level\r\n\r\nimport org.apache.spark.SparkConf\r\nimport org.apache.spark.SparkContext\r\nimport org.apache.spark.SparkContext._\r\nimport org.apache.spark.rdd._\r\nimport org.apache.spark.mllib.recommendation.{ALS, Rating, MatrixFactorizationModel}","user":"anonymous","dateUpdated":"2020-03-01T13:07:18+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import java.io.File\nimport scala.io.Source\nimport org.apache.log4j.Logger\nimport org.apache.log4j.Level\nimport org.apache.spark.SparkConf\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.rdd._\nimport org.apache.spark.mllib.recommendation.{ALS, Rating, MatrixFactorizationModel}\n"}]},"apps":[],"jobName":"paragraph_1583067903412_1812685885","id":"20200301-130503_326518195","dateCreated":"2020-03-01T13:05:03+0000","dateStarted":"2020-03-01T13:07:18+0000","dateFinished":"2020-03-01T13:07:18+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:172"},{"text":"val movieLensHomeDir = \"s3://emr.examples/movieLens/\"\r\n\r\nval movies = sc.textFile(movieLensHomeDir + \"movies.dat\").map { line =>\r\n  val fields = line.split(\"::\")\r\n  // format: (movieId, movieName)\r\n  (fields(0).toInt, fields(1))\r\n}.collect.toMap\r\n\r\nval ratings = sc.textFile(movieLensHomeDir + \"ratings.dat\").map { line =>\r\n  val fields = line.split(\"::\")\r\n  // format: (timestamp % 10, Rating(userId, movieId, rating))\r\n  (fields(3).toLong % 10, Rating(fields(0).toInt, fields(1).toInt, fields(2).toDouble))\r\n}","user":"anonymous","dateUpdated":"2020-03-01T13:35:17+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"java.io.IOException: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: 1DA6EB4168BA661D; S3 Extended Request ID: pzih5DgR9pIoMuvtjKwQh0YR9k141wxahBF/GHzU1KJEWc6lBja8tJNJ4+YdFEyNeBx2X2kvnSs=), S3 Extended Request ID: pzih5DgR9pIoMuvtjKwQh0YR9k141wxahBF/GHzU1KJEWc6lBja8tJNJ4+YdFEyNeBx2X2kvnSs=\n  at com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:292)\n  at com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:883)\n  at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:64)\n  at org.apache.hadoop.fs.Globber.doGlob(Globber.java:269)\n  at org.apache.hadoop.fs.Globber.glob(Globber.java:148)\n  at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1760)\n  at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.globStatus(EmrFileSystem.java:408)\n  at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:238)\n  at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:208)\n  at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:288)\n  at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:204)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n  at org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n  ... 59 elided\nCaused by: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: 1DA6EB4168BA661D; S3 Extended Request ID: pzih5DgR9pIoMuvtjKwQh0YR9k141wxahBF/GHzU1KJEWc6lBja8tJNJ4+YdFEyNeBx2X2kvnSs=)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1712)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1367)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1113)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:770)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:744)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:726)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:686)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:668)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:532)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:512)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4926)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4872)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4866)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:924)\n  at com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:22)\n  at com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:8)\n  at com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor.execute(GlobalS3Executor.java:110)\n  at com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:189)\n  at com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:184)\n  at com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.listObjectsV2(AmazonS3LiteClient.java:75)\n  at com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:284)\n  ... 89 more\n"}]},"apps":[],"jobName":"paragraph_1583068036507_28452757","id":"20200301-130716_99420628","dateCreated":"2020-03-01T13:07:16+0000","dateStarted":"2020-03-01T13:35:17+0000","dateFinished":"2020-03-01T13:35:19+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:173"},{"text":"val movieLensHomeDir = \"s3://emr.examples/movieLens/\"\r\n\r\nval movies = sc.textFile(movieLensHomeDir + \"movies.dat\").map { line =>\r\n  val fields = line.split(\"::\")\r\n  // format: (movieId, movieName)\r\n  (fields(0).toInt, fields(1))\r\n}.collect.toMap","user":"anonymous","dateUpdated":"2020-03-01T13:56:17+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"java.io.IOException: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: CFE8F2492924962B; S3 Extended Request ID: 3Hya2TbVGZvz6L0Dal2xdQiazrc1peyMvo+azpKpt7hpXSeoueZL96eb5QWfPBEYdDLIqtBBoww=), S3 Extended Request ID: 3Hya2TbVGZvz6L0Dal2xdQiazrc1peyMvo+azpKpt7hpXSeoueZL96eb5QWfPBEYdDLIqtBBoww=\n  at com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:292)\n  at com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:883)\n  at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:64)\n  at org.apache.hadoop.fs.Globber.doGlob(Globber.java:269)\n  at org.apache.hadoop.fs.Globber.glob(Globber.java:148)\n  at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1760)\n  at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.globStatus(EmrFileSystem.java:408)\n  at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:238)\n  at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:208)\n  at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:288)\n  at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:204)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n  at org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n  ... 59 elided\nCaused by: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: CFE8F2492924962B; S3 Extended Request ID: 3Hya2TbVGZvz6L0Dal2xdQiazrc1peyMvo+azpKpt7hpXSeoueZL96eb5QWfPBEYdDLIqtBBoww=)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1712)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1367)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1113)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:770)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:744)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:726)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:686)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:668)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:532)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:512)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4926)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4872)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4866)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:924)\n  at com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:22)\n  at com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:8)\n  at com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor.execute(GlobalS3Executor.java:110)\n  at com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:189)\n  at com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:184)\n  at com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.listObjectsV2(AmazonS3LiteClient.java:75)\n  at com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:284)\n  ... 89 more\n"}]},"apps":[],"jobName":"paragraph_1583069717515_-1375904577","id":"20200301-133517_1458249189","dateCreated":"2020-03-01T13:35:17+0000","dateStarted":"2020-03-01T13:56:19+0000","dateFinished":"2020-03-01T13:56:22+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:174"},{"text":"val movieLensHomeDir = \"s3://hashan-assignment-zepplin/movieLens/\"\r\n\r\nval movies = sc.textFile(movieLensHomeDir + \"movies.dat\").map { line =>\r\n  val fields = line.split(\"::\")\r\n  // format: (movieId, movieName)\r\n  (fields(0).toInt, fields(1))\r\n}.collect.toMap\r\n\r\nval ratings = sc.textFile(movieLensHomeDir + \"ratings.dat\").map { line =>\r\n  val fields = line.split(\"::\")\r\n  // format: (timestamp % 10, Rating(userId, movieId, rating))\r\n  (fields(3).toLong % 10, Rating(fields(0).toInt, fields(1).toInt, fields(2).toDouble))\r\n}","user":"anonymous","dateUpdated":"2020-03-01T14:20:19+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"movieLensHomeDir: String = s3://hashan-assignment-zepplin/movieLens/\nmovies: scala.collection.immutable.Map[Int,String] = Map(2163 -> Attack of the Killer Tomatoes! (1980), 8607 -> Tokyo Godfathers (2003), 645 -> Nelly & Monsieur Arnaud (1995), 42900 -> Cul-de-sac (1966), 892 -> Twelfth Night (1996), 69 -> Friday (1995), 53550 -> Rescue Dawn (2006), 37830 -> Final Fantasy VII: Advent Children (2004), 5385 -> Last Waltz, The (1978), 5810 -> 8 Mile (2002), 7375 -> Prince & Me, The (2004), 5659 -> Rocking Horse Winner, The (1950), 2199 -> Phoenix (1998), 8062 -> Dahmer (2002), 3021 -> Funhouse, The (1981), 8536 -> Intended, The (2002), 5437 -> Manhattan Project, The (1986), 1322 -> Amityville 1992: It's About Time (1992), 1665 -> Bean (1997), 5509 -> Biggie and Tupac (2002), 5686 -> Russia..."}]},"apps":[],"jobName":"paragraph_1583070975265_32730381","id":"20200301-135615_419177292","dateCreated":"2020-03-01T13:56:15+0000","dateStarted":"2020-03-01T14:20:19+0000","dateFinished":"2020-03-01T14:20:27+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:175"},{"text":"val numRatings = ratings.count\r\nval numUsers = ratings.map(_._2.user).distinct.count\r\nval numMovies = ratings.map(_._2.product).distinct.count\r\n\r\nprintln(\"Got \" + numRatings + \" ratings from \"\r\n  + numUsers + \" users on \" + numMovies + \" movies.\")","user":"anonymous","dateUpdated":"2020-03-01T14:21:27+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"java.io.IOException: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: F1F409D3A6025D47; S3 Extended Request ID: IFWwIlhCrX0DSqCOy2BPfHkU0Fxw9X6XBmWXCFUh3/q1azQhtsRcexhbLeG+VyRUvWIUXRoCOHU=), S3 Extended Request ID: IFWwIlhCrX0DSqCOy2BPfHkU0Fxw9X6XBmWXCFUh3/q1azQhtsRcexhbLeG+VyRUvWIUXRoCOHU=\n  at com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:292)\n  at com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:883)\n  at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:64)\n  at org.apache.hadoop.fs.Globber.doGlob(Globber.java:269)\n  at org.apache.hadoop.fs.Globber.glob(Globber.java:148)\n  at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1760)\n  at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.globStatus(EmrFileSystem.java:408)\n  at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:238)\n  at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:208)\n  at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:288)\n  at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:204)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n  at org.apache.spark.rdd.RDD.count(RDD.scala:1168)\n  ... 59 elided\nCaused by: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: F1F409D3A6025D47; S3 Extended Request ID: IFWwIlhCrX0DSqCOy2BPfHkU0Fxw9X6XBmWXCFUh3/q1azQhtsRcexhbLeG+VyRUvWIUXRoCOHU=)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1712)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1367)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1113)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:770)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:744)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:726)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:686)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:668)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:532)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:512)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4926)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4872)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4866)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:924)\n  at com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:22)\n  at com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:8)\n  at com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor.execute(GlobalS3Executor.java:110)\n  at com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:189)\n  at com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:184)\n  at com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.listObjectsV2(AmazonS3LiteClient.java:75)\n  at com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:284)\n  ... 85 more\n"}]},"apps":[],"jobName":"paragraph_1583072419135_802620161","id":"20200301-142019_649989565","dateCreated":"2020-03-01T14:20:19+0000","dateStarted":"2020-03-01T14:21:27+0000","dateFinished":"2020-03-01T14:21:27+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:176"},{"text":"val movieLensHomeDir = \"s3://hashan-assignment-zepplin/movieLens/\"\r\n\r\nval movies = sc.textFile(movieLensHomeDir + \"movies.dat\").map { line =>\r\n  val fields = line.split(\"::\")\r\n  // format: (movieId, movieName)\r\n  (fields(0).toInt, fields(1))\r\n}.collect.toMap\r\n\r\nval ratings = sc.textFile(movieLensHomeDir + \"ratings.dat\").map { line =>\r\n  val fields = line.split(\"::\")\r\n  // format: (timestamp % 10, Rating(userId, movieId, rating))\r\n  (fields(3).toLong % 10, Rating(fields(0).toInt, fields(1).toInt, fields(2).toDouble))\r\n}","user":"anonymous","dateUpdated":"2020-03-01T14:28:12+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"movieLensHomeDir: String = s3://hashan-assignment-zepplin/movieLens/\nmovies: scala.collection.immutable.Map[Int,String] = Map(2163 -> Attack of the Killer Tomatoes! (1980), 8607 -> Tokyo Godfathers (2003), 645 -> Nelly & Monsieur Arnaud (1995), 42900 -> Cul-de-sac (1966), 892 -> Twelfth Night (1996), 69 -> Friday (1995), 53550 -> Rescue Dawn (2006), 37830 -> Final Fantasy VII: Advent Children (2004), 5385 -> Last Waltz, The (1978), 5810 -> 8 Mile (2002), 7375 -> Prince & Me, The (2004), 5659 -> Rocking Horse Winner, The (1950), 2199 -> Phoenix (1998), 8062 -> Dahmer (2002), 3021 -> Funhouse, The (1981), 8536 -> Intended, The (2002), 5437 -> Manhattan Project, The (1986), 1322 -> Amityville 1992: It's About Time (1992), 1665 -> Bean (1997), 5509 -> Biggie and Tupac (2002), 5686 -> Russia..."}]},"apps":[],"jobName":"paragraph_1583072487420_1509652255","id":"20200301-142127_1281778564","dateCreated":"2020-03-01T14:21:27+0000","dateStarted":"2020-03-01T14:28:12+0000","dateFinished":"2020-03-01T14:28:20+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:177"},{"text":"val numRatings = ratings.count\r\nval numUsers = ratings.map(_._2.user).distinct.count\r\nval numMovies = ratings.map(_._2.product).distinct.count\r\n\r\nprintln(\"Got \" + numRatings + \" ratings from \"\r\n  + numUsers + \" users on \" + numMovies + \" movies.\")","user":"anonymous","dateUpdated":"2020-03-01T14:31:29+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"java.io.IOException: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: 67A52D9F240809BF; S3 Extended Request ID: MwaM0UStJRbsP6YP2o2PXWM+yvFyBPRRMFhBpC6VX2ETbeBmopO1VZ8ARbJem7WCm7hiGUyXc6g=), S3 Extended Request ID: MwaM0UStJRbsP6YP2o2PXWM+yvFyBPRRMFhBpC6VX2ETbeBmopO1VZ8ARbJem7WCm7hiGUyXc6g=\n  at com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:292)\n  at com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:883)\n  at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:64)\n  at org.apache.hadoop.fs.Globber.doGlob(Globber.java:269)\n  at org.apache.hadoop.fs.Globber.glob(Globber.java:148)\n  at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1760)\n  at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.globStatus(EmrFileSystem.java:408)\n  at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:238)\n  at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:208)\n  at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:288)\n  at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:204)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n  at org.apache.spark.rdd.RDD.count(RDD.scala:1168)\n  ... 59 elided\nCaused by: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: 67A52D9F240809BF; S3 Extended Request ID: MwaM0UStJRbsP6YP2o2PXWM+yvFyBPRRMFhBpC6VX2ETbeBmopO1VZ8ARbJem7WCm7hiGUyXc6g=)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1712)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1367)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1113)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:770)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:744)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:726)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:686)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:668)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:532)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:512)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4926)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4872)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4866)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:924)\n  at com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:22)\n  at com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:8)\n  at com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor.execute(GlobalS3Executor.java:110)\n  at com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:189)\n  at com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:184)\n  at com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.listObjectsV2(AmazonS3LiteClient.java:75)\n  at com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:284)\n  ... 85 more\n"}]},"apps":[],"jobName":"paragraph_1583072892850_1273250406","id":"20200301-142812_1871974417","dateCreated":"2020-03-01T14:28:12+0000","dateStarted":"2020-03-01T14:31:29+0000","dateFinished":"2020-03-01T14:31:30+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:178"},{"text":"val training = ratings.filter(x => x._1 < 6)\r\n  .values\r\n  .cache()\r\nval validation = ratings.filter(x => x._1 >= 6 && x._1 < 8)\r\n  .values\r\n  .cache()\r\nval test = ratings.filter(x => x._1 >= 8).values.cache()\r\n\r\nval numTraining = training.count()\r\nval numValidation = validation.count()\r\nval numTest = test.count()\r\n\r\nprintln(\"Training: \" + numTraining + \", validation: \" + numValidation + \", test: \" + numTest)","user":"anonymous","dateUpdated":"2020-03-01T14:32:20+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"java.io.IOException: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: 1B71699F376202D9; S3 Extended Request ID: DjQ2+kzaK47lrJocsQQ49f5eaHPJ/ACd5J6GlHuT8MHXc3j7yw/IjYTZw3zj2EdFS6COYH4b3I0=), S3 Extended Request ID: DjQ2+kzaK47lrJocsQQ49f5eaHPJ/ACd5J6GlHuT8MHXc3j7yw/IjYTZw3zj2EdFS6COYH4b3I0=\n  at com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:292)\n  at com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:883)\n  at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:64)\n  at org.apache.hadoop.fs.Globber.doGlob(Globber.java:269)\n  at org.apache.hadoop.fs.Globber.glob(Globber.java:148)\n  at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1760)\n  at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.globStatus(EmrFileSystem.java:408)\n  at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:238)\n  at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:208)\n  at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:288)\n  at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:204)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n  at org.apache.spark.rdd.RDD.count(RDD.scala:1168)\n  ... 59 elided\nCaused by: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: 1B71699F376202D9; S3 Extended Request ID: DjQ2+kzaK47lrJocsQQ49f5eaHPJ/ACd5J6GlHuT8MHXc3j7yw/IjYTZw3zj2EdFS6COYH4b3I0=)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1712)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1367)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1113)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:770)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:744)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:726)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:686)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:668)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:532)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:512)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4926)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4872)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4866)\n  at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:924)\n  at com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:22)\n  at com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:8)\n  at com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor.execute(GlobalS3Executor.java:110)\n  at com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:189)\n  at com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:184)\n  at com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.listObjectsV2(AmazonS3LiteClient.java:75)\n  at com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:284)\n  ... 95 more\n"}]},"apps":[],"jobName":"paragraph_1583073089570_-425224983","id":"20200301-143129_1318344937","dateCreated":"2020-03-01T14:31:29+0000","dateStarted":"2020-03-01T14:32:20+0000","dateFinished":"2020-03-01T14:32:21+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:179"},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1583073140975_-1478180095","id":"20200301-143220_1010541043","dateCreated":"2020-03-01T14:32:20+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:180","text":"val movieLensHomeDir = \"s3://wiiszippline/data_set/\"\r\n\r\nval movies = sc.textFile(movieLensHomeDir + \"movies.dat\").map { line =>\r\n  val fields = line.split(\"::\")\r\n  // format: (movieId, movieName)\r\n  (fields(0).toInt, fields(1))\r\n}.collect.toMap\r\n\r\nval ratings = sc.textFile(movieLensHomeDir + \"ratings.dat\").map { line =>\r\n  val fields = line.split(\"::\")\r\n  // format: (timestamp % 10, Rating(userId, movieId, rating))\r\n  (fields(3).toLong % 10, Rating(fields(0).toInt, fields(1).toInt, fields(2).toDouble))\r\n}","dateUpdated":"2020-03-01T15:57:25+0000","dateFinished":"2020-03-01T15:57:33+0000","dateStarted":"2020-03-01T15:57:25+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"movieLensHomeDir: String = s3://wiiszippline/data_set/\nmovies: scala.collection.immutable.Map[Int,String] = Map(2163 -> Attack of the Killer Tomatoes! (1980), 8607 -> Tokyo Godfathers (2003), 645 -> Nelly & Monsieur Arnaud (1995), 42900 -> Cul-de-sac (1966), 892 -> Twelfth Night (1996), 69 -> Friday (1995), 53550 -> Rescue Dawn (2006), 37830 -> Final Fantasy VII: Advent Children (2004), 5385 -> Last Waltz, The (1978), 5810 -> 8 Mile (2002), 7375 -> Prince & Me, The (2004), 5659 -> Rocking Horse Winner, The (1950), 2199 -> Phoenix (1998), 8062 -> Dahmer (2002), 3021 -> Funhouse, The (1981), 8536 -> Intended, The (2002), 5437 -> Manhattan Project, The (1986), 1322 -> Amityville 1992: It's About Time (1992), 1665 -> Bean (1997), 5509 -> Biggie and Tupac (2002), 5686 -> Russian Ark (Russkiy..."}]}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1583078245474_-2131583114","id":"20200301-155725_1606955507","dateCreated":"2020-03-01T15:57:25+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:962","text":"val numRatings = ratings.count\r\nval numUsers = ratings.map(_._2.user).distinct.count\r\nval numMovies = ratings.map(_._2.product).distinct.count\r\n\r\nprintln(\"Got \" + numRatings + \" ratings from \"\r\n  + numUsers + \" users on \" + numMovies + \" movies.\")","dateUpdated":"2020-03-01T15:59:02+0000","dateFinished":"2020-03-01T15:59:30+0000","dateStarted":"2020-03-01T15:59:02+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Got 10000054 ratings from 69878 users on 10677 movies.\nnumRatings: Long = 10000054\nnumUsers: Long = 69878\nnumMovies: Long = 10677\n"}]}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1583078342529_-143730569","id":"20200301-155902_1269091756","dateCreated":"2020-03-01T15:59:02+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1066","text":"val training = ratings.filter(x => x._1 < 6)\r\n  .values\r\n  .cache()\r\nval validation = ratings.filter(x => x._1 >= 6 && x._1 < 8)\r\n  .values\r\n  .cache()\r\nval test = ratings.filter(x => x._1 >= 8).values.cache()\r\n\r\nval numTraining = training.count()\r\nval numValidation = validation.count()\r\nval numTest = test.count()\r\n\r\nprintln(\"Training: \" + numTraining + \", validation: \" + numValidation + \", test: \" + numTest)\r\n","dateUpdated":"2020-03-01T16:00:22+0000","dateFinished":"2020-03-01T16:00:46+0000","dateStarted":"2020-03-01T16:00:22+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Training: 6002473, validation: 1999675, test: 1997906\ntraining: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] = MapPartitionsRDD[110] at values at <console>:66\nvalidation: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] = MapPartitionsRDD[112] at values at <console>:69\ntest: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] = MapPartitionsRDD[114] at values at <console>:71\nnumTraining: Long = 6002473\nnumValidation: Long = 1999675\nnumTest: Long = 1997906\n"}]}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1583078422094_1680729489","id":"20200301-160022_695257176","dateCreated":"2020-03-01T16:00:22+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1178","text":"/** Compute RMSE (Root Mean Squared Error). */\r\ndef computeRmse(model: MatrixFactorizationModel, data: RDD[Rating], n: Long): Double = {\r\n    val predictions: RDD[Rating] = model.predict(data.map(x => (x.user, x.product)))\r\n    val predictionsAndRatings = predictions.map(x => ((x.user, x.product), x.rating))\r\n    .join(data.map(x => ((x.user, x.product), x.rating))).values\r\n    math.sqrt(predictionsAndRatings.map(x => (x._1 - x._2) * (x._1 - x._2)).reduce(_ + _) / n)\r\n}","dateUpdated":"2020-03-01T16:01:43+0000","dateFinished":"2020-03-01T16:01:44+0000","dateStarted":"2020-03-01T16:01:43+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"computeRmse: (model: org.apache.spark.mllib.recommendation.MatrixFactorizationModel, data: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating], n: Long)Double\n"}]}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1583078503576_1320567989","id":"20200301-160143_1989781916","dateCreated":"2020-03-01T16:01:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1275","text":"val ranks = List(8, 12)\r\nval lambdas = List(0.1, 10.0)\r\nval numIters = List(10, 20)\r\nvar bestModel: Option[MatrixFactorizationModel] = None\r\nvar bestValidationRmse = Double.MaxValue\r\nvar bestRank = 0\r\nvar bestLambda = -1.0\r\nvar bestNumIter = -1\r\nfor (rank <- ranks; lambda <- lambdas; numIter <- numIters) {\r\n  val model = ALS.train(training, rank, numIter, lambda)\r\n  val validationRmse = computeRmse(model, validation, numValidation)\r\n  println(\"RMSE (validation) = \" + validationRmse + \" for the model trained with rank = \" \r\n    + rank + \", lambda = \" + lambda + \", and numIter = \" + numIter + \".\")\r\n  if (validationRmse < bestValidationRmse) {\r\n    bestModel = Some(model)\r\n    bestValidationRmse = validationRmse\r\n    bestRank = rank\r\n    bestLambda = lambda\r\n    bestNumIter = numIter\r\n  }\r\n}","dateUpdated":"2020-03-01T16:02:33+0000","dateFinished":"2020-03-01T16:09:47+0000","dateStarted":"2020-03-01T16:02:33+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"RMSE (validation) = 0.8230715711943087 for the model trained with rank = 8, lambda = 0.1, and numIter = 10.\nRMSE (validation) = 0.8181127621510035 for the model trained with rank = 8, lambda = 0.1, and numIter = 20.\nRMSE (validation) = 3.667982949261605 for the model trained with rank = 8, lambda = 10.0, and numIter = 10.\nRMSE (validation) = 3.667982949261605 for the model trained with rank = 8, lambda = 10.0, and numIter = 20.\nRMSE (validation) = 0.819514985361677 for the model trained with rank = 12, lambda = 0.1, and numIter = 10.\nRMSE (validation) = 0.8154544932902966 for the model trained with rank = 12, lambda = 0.1, and numIter = 20.\nRMSE (validation) = 3.667982949261605 for the model trained with rank = 12, lambda = 10.0, and numIter = 10.\nRMSE (validation) = 3.667982949261605 for the model trained with rank = 12, lambda = 10.0, and numIter = 20.\nranks: List[Int] = List(8, 12)\nlambdas: List[Double] = List(0.1, 10.0)\nnumIters: List[Int] = List(10, 20)\nbestModel: Option[org.apache.spark.mllib.recommendation.MatrixFactorizationModel] = Some(org.apache.spark.mllib.recommendation.MatrixFactorizationModel@6d446a55)\nbestValidationRmse: Double = 0.8154544932902966\nbestRank: Int = 12\nbestLambda: Double = 0.1\nbestNumIter: Int = 20\n"}]}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1583078553318_-1450321305","id":"20200301-160233_534967895","dateCreated":"2020-03-01T16:02:33+0000","status":"ERROR","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1378","text":"// evaluate the best model on the test set\r\nval testRmse = computeRmse(bestModel.get, test, numTest)\r\n\r\nprintln(\"The best model was trained with rank = \" + bestRank + \" and lambda = \" + bestLambda\r\n  + \", and numIter = \" + bestNumIter + \", and its RMSE on the test set is\r\n  \" + testRmse + \".\")","dateUpdated":"2020-03-01T16:13:59+0000","dateFinished":"2020-03-01T16:13:59+0000","dateStarted":"2020-03-01T16:13:59+0000","results":{"code":"ERROR","msg":[{"type":"TEXT","data":"<console>:5: error: unclosed string literal\n  + \", and numIter = \" + bestNumIter + \", and its RMSE on the test set is\n                                       ^\n<console>:6: error: unclosed string literal\n  \" + testRmse + \".\")\n                   ^\n"}]}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1583079239555_2103372026","id":"20200301-161359_1912350888","dateCreated":"2020-03-01T16:13:59+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1483","text":"val testRmse = computeRmse(bestModel.get, test, numTest)\r\n\r\nprintln(\"The best model was trained with rank = \" + bestRank + \" and lambda = \" + bestLambda\r\n  + \", and numIter = \" + bestNumIter + \", and its RMSE on the test set is\" + testRmse + \".\")","dateUpdated":"2020-03-01T16:14:58+0000","dateFinished":"2020-03-01T16:15:07+0000","dateStarted":"2020-03-01T16:14:58+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"The best model was trained with rank = 12 and lambda = 0.1, and numIter = 20, and its RMSE on the test set is0.8155558029219319.\ntestRmse: Double = 0.8155558029219319\n"}]}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1583079298168_-1675210510","id":"20200301-161458_1632395706","dateCreated":"2020-03-01T16:14:58+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1577","text":"// create a naive baseline and compare it with the best model\r\nval meanRating = training.union(validation).map(_.rating).mean\r\nval baselineRmse = \r\n  math.sqrt(test.map(x => (meanRating - x.rating) * (meanRating - x.rating)).mean)\r\nval improvement = (baselineRmse - testRmse) / baselineRmse * 100\r\nprintln(\"The best model improves the baseline by \" + \"%1.2f\".format(improvement) + \"%.\")","dateUpdated":"2020-03-01T16:22:44+0000","dateFinished":"2020-03-01T16:22:45+0000","dateStarted":"2020-03-01T16:22:44+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"The best model improves the baseline by 23.05%.\nmeanRating: Double = 3.5123623057208624\nbaselineRmse: Double = 1.0597828264660583\nimprovement: Double = 23.045006716944407\n"}]}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1583079764450_-649700146","id":"20200301-162244_1232170339","dateCreated":"2020-03-01T16:22:44+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1682","text":"val candidates = sc.parallelize(movies.keys.toSeq)\r\nval recommendations = bestModel.get\r\n  .predict(candidates.map((100, _)))\r\n  .collect()\r\n  .sortBy(- _.rating)\r\n  .take(10)\r\n\r\nvar i = 1\r\nprintln(\"Movies recommended for you:\")\r\nrecommendations.foreach { r =>\r\n  println(\"%2d\".format(i) + \": \" + movies(r.product))\r\n  i += 1\r\n}","dateUpdated":"2020-03-01T16:26:10+0000","dateFinished":"2020-03-01T16:26:11+0000","dateStarted":"2020-03-01T16:26:10+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Movies recommended for you:\n 1: Eve and the Fire Horse (2005)\n 2: Maradona by Kusturica (2008)\n 3: Power of Nightmares: The Rise of the Politics of Fear, The (2004)\n 4: Low Life, The (1995)\n 5: Shadows of Forgotten Ancestors (1964)\n 6: Tunnel, The (Der Tunnel) (2001)\n 7: Hospital (1970)\n 8: Pulp Fiction (1994)\n 9: Unreasonable Man, An (2006)\n10: Godfather, The (1972)\ncandidates: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[2649] at parallelize at <console>:69\nrecommendations: Array[org.apache.spark.mllib.recommendation.Rating] = Array(Rating(100,60983,4.30997483375307), Rating(100,61742,3.832229604260876), Rating(100,53883,3.7585418374232775), Rating(100,32090,3.7440851193386155), Rating(100,42783,3.7207276330936763), Rating(100,27376,3.516778054844), Rating(100,64280,3.5019702397499315), Rating(100,296,3.4963546913609065), Rating(100,55156,3.486455147042859), Rating(100,858,3.4596156713337507))\ni: Int = 11\n"}]}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1583079970378_98422960","id":"20200301-162610_1078995441","dateCreated":"2020-03-01T16:26:10+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1785","text":"val moviesWithGenres = sc.textFile(movieLensHomeDir + \"movies.dat\").map { line =>\r\n  val fields = line.split(\"::\")\r\n  // format: (movieId, movieName, genre information)\r\n  (fields(0).toInt, fields(2))\r\n}.collect.toMap","dateUpdated":"2020-03-01T16:27:25+0000","dateFinished":"2020-03-01T16:27:26+0000","dateStarted":"2020-03-01T16:27:25+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"moviesWithGenres: scala.collection.immutable.Map[Int,String] = Map(2163 -> Comedy|Horror, 8607 -> Adventure|Animation|Drama, 645 -> Drama, 42900 -> Comedy|Crime|Drama|Thriller, 892 -> Comedy|Drama|Romance, 69 -> Comedy, 53550 -> Action|Adventure|Drama|War, 37830 -> Action|Adventure|Animation|Fantasy|Sci-Fi, 5385 -> Documentary, 5810 -> Drama, 7375 -> Comedy|Romance, 5659 -> Drama|Horror, 2199 -> Crime|Drama, 8062 -> Drama|Horror|Thriller, 3021 -> Horror, 8536 -> Drama|Thriller, 5437 -> Comedy|Thriller, 1322 -> Horror, 1665 -> Comedy, 5509 -> Documentary, 5686 -> Drama|Fantasy|War, 1036 -> Action|Crime|Thriller, 2822 -> Adventure|Romance, 7304 -> Animation|Comedy|Fantasy|Musical, 54999 -> Action|Adventure|Thriller, 2630 -> Drama, 6085 -> Comedy|Drama, 3873 -> Comedy|Western, 4188 -> Chil..."}]}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1583080045575_2119694808","id":"20200301-162725_601909867","dateCreated":"2020-03-01T16:27:25+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1893","text":"val comedyMovies = moviesWithGenres.filter(_._2.matches(\".*Comedy.*\")).keys\r\nval candidates = sc.parallelize(comedyMovies.toSeq)\r\nval recommendations = bestModel.get\r\n  .predict(candidates.map((100, _)))\r\n  .collect()\r\n  .sortBy(- _.rating)\r\n  .take(5)\r\n\r\nvar i = 1\r\nprintln(\"Comedy Movies recommended for you:\")\r\nrecommendations.foreach { r =>\r\n  println(\"%2d\".format(i) + \": \" + movies(r.product))\r\n  i += 1\r\n}\r\n","dateUpdated":"2020-03-01T16:28:42+0000","dateFinished":"2020-03-01T16:28:42+0000","dateStarted":"2020-03-01T16:28:42+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Comedy Movies recommended for you:\n 1: Pulp Fiction (1994)\n 2: Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1964)\n 3: Yojimbo (1961)\n 4: Mafioso (1962)\n 5: One Flew Over the Cuckoo's Nest (1975)\ncomedyMovies: Iterable[Int] = Set(2163, 42900, 892, 69, 7375, 5437, 1665, 7304, 6085, 3873, 26413, 4201, 4447, 33004, 3962, 5422, 5469, 3944, 6387, 3883, 62851, 5116, 4094, 6167, 5088, 2889, 59858, 2295, 2306, 4571, 5857, 4464, 101, 2109, 1454, 4909, 2031, 5896, 59625, 2072, 8663, 4062, 3399, 54256, 33675, 6544, 4169, 4899, 53578, 6712, 55020, 5950, 3167, 31160, 4183, 909, 4290, 3477, 333, 3979, 2463, 3397, 49110, 3581, 8784, 3830, 6317, 518, 7990, 2499, 8843, 1083, 468, 54193, 5205, 6172, 4015, 26842, 234, 6690, 2331, 3566, 4728, 6954, 4877, 6014, 5582, 4992, 5131, 6374, 88, 50354, 47047, 32289, 352, 53993, 33145, 1855, 45722, 5454, 56176, 1211, 3990, 7888, 4714, 1158, 582, 762, 3072, 8883, 1005, 5141, 115, 6944, 3317, 5168, 4500, 65027, 7409, 5718, 34018, 37384, 46976, 276, 2622, 4402..."}]}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1583080122320_1699352852","id":"20200301-162842_1128625233","dateCreated":"2020-03-01T16:28:42+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1998","text":"bestModel.get.save(sc, \"s3://wiiszippline/data_set/recommendation\")\r\nval sameModel = MatrixFactorizationModel.load(sc,  \"s3://wiiszippline/data_set/recommendation\")","dateUpdated":"2020-03-01T16:33:09+0000","dateFinished":"2020-03-01T16:33:16+0000","dateStarted":"2020-03-01T16:33:09+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"sameModel: org.apache.spark.mllib.recommendation.MatrixFactorizationModel = org.apache.spark.mllib.recommendation.MatrixFactorizationModel@598762e\n"}]}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1583080389571_415820465","id":"20200301-163309_1830710824","dateCreated":"2020-03-01T16:33:09+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2106"}],"name":"assignment_march01","id":"2F4YK2ZB5","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}